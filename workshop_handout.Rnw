\documentclass{article} %\documentclass[a4paper,12pt,twoside]{memoir}
%\usepackage{btp}
%
% Example Workshop Handout with title page and a section demonstrating
% the different environments defined in the top matter


% if trainermanual is true, then include extra info in the report
\newif\iftrainermanual
\trainermanualtrue

\usepackage{url}
\usepackage{graphicx}
\usepackage[paper=a4paper,top=35mm,left=25mm,right=25mm,bottom=30mm]{geometry}

% Set some Workshop specific info
\iftrainermanual
\title{TRAINER MANUAL for R and rqtl workshop}
\else
\title{Introductory workshop for R and R/qtl}
\fi
\date{May 2014}
\author{Angus Wallace and Julian Taylor}



\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
%\setWorkshopTitle{Introductory R workshop, rqtl}
%\setWorkshopVenue{ACPFG}
%\setWorkshopDate{May 2014}
%\setWorkshopAuthor{Angus Wallace}

%% this document can be rendered in R by typing:
%		CODEDIR <- Sys.getenv("CODEDIR")
%		DATADIR <- Sys.getenv("DATADIR")
%		setwd("~/R_workshop_handout/")
%		library(knitr)
%		Sys.setenv(TEXINPUTS=getwd(),
%				     BIBINPUTS=getwd(),
%				     BSTINPUTS=getwd())
%		knit2pdf("~/R_workshop_handout/workshop_handout.Rnw", output=paste(DATADIR,"/pdfs/R_workshop_handout.tex",sep=""))

\setlength{\parskip}{2ex}

<<setup, echo=FALSE, include=FALSE, cache=FALSE>>=
opts_chunk$set(tidy=TRUE, tidy.opts=list(width.cutoff=70))
CODEDIR <- Sys.getenv("CODEDIR")
DATADIR <- Sys.getenv("DATADIR")
library(xtable)
library(reshape)
library(plyr)
library(stringr)
library(made4)
options(width=70)
@



%
% Workshop Title Page
%
%\workshoptitlepage

\section{This workshop}
\subsection{Aims}
Brief introduction to R\\
Basic use

\begin{enumerate}
\item{fundamental use}
\item{data types}
\item{read in data}
\item{quality-check and visualise data}
\item{write out data}
\item{rqtl}
\end{enumerate}
\section{How to read this document}

Each section in this workshop introduces a new concept or tool. Each section contains some examples and is followed by some problems for you to work through to cement your understanding. The facilitators are here to help you with these. It's hopefully clear that the purpose of these problems is to understand them, rather than get the "correct" pattern of letters and numbers on the screen!

\subsection{Code}
To use R, one must write code. This document contains code for different purposes, and it is formatted in a characteristic and consistent way throughout.
<<code_eg, eval=TRUE, cache=FALSE, result='asis',background='#FDFDFD'>>=
# boxes like this contain example code

# comments start with a hash 
# (like this line, which is not run)
print("this line has no hash at the start, so is run")

# Also, note that output generated by R has a double-comment

# There are three kinds of problems for you to attempt:
@

\subsubsection{Exercises (aka problems)}
<<ex_work1, eval=TRUE, cache=FALSE, result='asis', size="large",background='#D7FFD7', highlight=FALSE>>=
# You should try to complete all exercises in green
@

<<ex_work2, eval=TRUE, cache=FALSE, result='asis', size="large",background='#FfEDD9', highlight=FALSE>>= 
# If you are progressing well, attempt exercises in orange also (as an extension)
@

<<ex_work3, eval=TRUE, cache=FALSE, result='asis', size="large",background='#FFDDDD', highlight=FALSE>>=
# Purple exercises are designed to be difficult. If you can do these, great. Otherwise, you might want to think about them later.
@

\iftrainermanual
\subsubsection{Trainer solutions}
<<ex_soln, eval=TRUE, cache=FALSE, result='asis', size="large",background='#DDFFFF', highlight=FALSE>>=
# Trainer solutions to the Exercises are included on a blue background
@
\fi

\part{Working in R}
\section{Fundamentals}

\subsection{Functions}
Definition:
R uses \textit{functions} to get things done. Think of them as tools.
The functions are collated into \textit{libraries} (aka \textit{packages}) that can be loaded when needed. Think of them as toolboxes.
For example, in the same way that a carpenter may keep all their chisels together in a toolbox, a particular R library/package might contain functions to allow the plotting of data. \\
It is possible to write functions for commonly performed activities, and this is very useful (and acheivable!), but it is outside the scope of this workshop.

\subsection{Using libraries}
So, libraries/packages contain collections of functions, which are what get things done. When one first opens R, the \textit{base} library is automatically loaded, and provides the fundamental functions. \\
The following code shows the installation and loading of a library

<<libraries, eval=TRUE, cache=FALSE, result='asis',background='#FDFDFD'>>=
# install a package
install.packages("ggplot2")
# load the package
library(ggplot2)

# note that when installing, one needs to quote the package name, but when loading it one does not 
@

\subsection{Working directory}
If you ask R to open a file without telling it where to look for that file, it will look in the current working directory. If it doesn't find the file there, it will report "no such file or directory". It is easy to query and change the working directory in R, by using the getwd() and setwd() functions. \\
\\
But it's even easier in RStudio. Just go \\
Session -- Set Working Directory -- Choose Directory \\
and choose where you want to be. 

Do this now, and select the Desktop.

\subsection{Getting help}
You will do this constantly!
Most functions in R have a help page that explains how to use the function. One can access this by typing (eg.) \\
?ggplot\\

(a question mark, followed by the function name. Note that the function's library must be loaded first)
If you are not really sure what you are looking for, you can search for a term, eg\\
??histogram\\

Which will bring up help files that contain the word histogram

I generally find it better to use Google, but be warned -- because its name is so short, "R" is a difficult term to search for -- try to be specific

\section{What makes R useful? What can it do?}

\section{Data}

\subsection{load some data, from a file, into R}

We don't want to type in data by hand. Usually, we have data in a spreadsheet that we want to analyse. There are two main ways you can do this:
\begin{enumerate}
\item{If you have a spreadsheet program, you can select data, and copy it to the clipboard. Typing \\
\texttt{myData <- read.table(file="clipboard")} \\
will read those data into a variable called myData
}
\item{This is handy, but if this is something you will do again and again, it's a bit cumbersome. It's also easy to make mistakes. A more robust alternative is to export the data from the excel file to a csv\footnote{ Click on (File - Save As) and select csv or "Comma separated" as the type of file.}}
\end{enumerate}


The exact way to read in the data is determined by the format of your csv file. eg,

\texttt{\\
sample, cultivar, height, tillers, grain, group\\
1,      RAC,      3.25,   5,       4,     drought\\
2,      EXCAL,    3.25,   5,       4,     drought\\
3,      RAC,      3.25,   5,       4,     control\\
4,      EXCAL,    3.25,   5,       4,     control} \\
Then you would read it in with the command:
<<csv_read_1, eval=FALSE, cache=FALSE, result='asis',background='#FDFDFD'>>=
setwd("~/R_workshop_handout")  # set the working directory (where R looks for files)
filename     <- "plex_dat_small.csv"   # define the filename
drought_data <- read.table(filename, header=TRUE, sep=",", 
                            stringsAsFactors=FALSE)
@

Note that factors are a type of data that we have not discussed. If you have a long vector of strings it can occupy a lot of memory. If the strings are repetative, a memory use can be reduced by using a lookup table of numbers to the strings (assign each unique string (the \textit{levels}) a number) and then storing only the numbers in the vector. These are called \textit{factors} in R.

Alternatively, if the csv file looked like this:\\
\texttt{\\
"sample"  "cultivar"  "height"  "tillers"  "grain"  "group"\\
1       "RAC"       3.25    5        4      "drought"\\
2       "EXCAL"     3.25    5        4      "drought"\\
3       "RAC"       3.25    5        4      "control"\\
4       "EXCAL"     3.25    5        4      "control"\\
}\\
Then you would read it in with the command:
<<csv_2, eval=FALSE, cache=FALSE, result='asis',background='#FDFDFD'>>=
drought_data <- read.table(filename, header=TRUE, sep=" ", quote="\"", stringsAsFactors=FALSE)
@

The idea here, is that we have a quote symbol within quotes. In other words, we tell R which character denotes a quotation. Because the quotation character itself has special meaning to R, we have to tell R that we really mean the character and are not just finishing the quotation. We do this by putting a backslash in front of the character. This is also called "escaping" the character. If we just had three quote symbols in a row (without the backslash) R would think we were opening and closing a quotation, and then opening another -- the second one would never be closed, which would cause an error.

%%% PROBLEMS
\subsubsection{Exercises}
Before you can work on these exercises, you will need to download some files. \\
\\
Go to \url{dropcanvas.com/0z3wq} and download the two files onto your Desktop
(right-click on each file, and "Save target as..." then save to the Desktop.)

%% generate the csv file:
% plex_dat <- read.table("~/R_workshop_handout/TA44.dat", header=TRUE, quote="\"")
% plex_dat$affy <- gsub("_[0-9]{7}","",row.names(plex_dat))
% plex_dat <- plex_dat[!duplicated(plex_dat$affy),]
% row.names(plex_dat) <- plex_dat$affy
% plex_dat <- plex_dat[!duplicated(plex_dat$affy),]
% plex_dat <- plex_dat[,-25]
% write.table(plex_dat,file="~/R_workshop_handout/plex_dat.dat")

<<readin_work1, eval=TRUE, cache=FALSE, result='asis', size="large",background='#D7FFD7', highlight=FALSE>>=
# There is a csv file that you can read. It is located at [plex_dat_small.csv]. 

# 1. Open this file in RStudio, using [FILE -> Open File]
# in the menu. This will open the file in a text-editor within  
# RStudio so that its contents and formatting can be viewed.

# 2. Read in the file, using read.table, to a new variable.

# 3. What are the column names?

# 4. What are the first 10 row names?

# 5.1. Extract rows 1 and 2

# 5.2. Find the difference between the two rows

@

<<readin_work2, eval=FALSE, cache=FALSE, result='asis', size="large",background='#FfEDD9', highlight=FALSE>>= 

# 6. list and plot all the data for probe "Ta.10026.1.A1_at"

# 7. create a histogram of the data in column "X2.WL711.susceptible.irrigated" try: ??histogram
@

<<readin_work3, eval=TRUE, cache=FALSE, result='asis', size="large",background='#FFDDDD', highlight=FALSE>>=
# 8. Plot the data in row 5 (use the [plot], [lines] and [t] (transpose) functions)
@

%%% SOLUTIONS
\iftrainermanual
\subsubsection{Trainer solutions}
<<readin_soln, eval=FALSE, cache=FALSE, result='hide', size="large",background='#DDFFFF', highlight=FALSE>>=
# 1. 

# 2. Read in the file, using read.table, to a new variable.
plex_dat <- read.table("plex_dat_small.dat", header=TRUE, quote="\"")

# 3. Make a list of the column names
names(plex_dat)

# 4. Make a list of the first 10 row names
row.names(plex_dat)[1:10]

# 5. Calculate the difference, at each sample, between rows 1 and 2
row1 <- plex_dat[1,]
row2 <- plex_dat[2,]
row1 - row2
# a bit hard to see like that, transposing can help
t(row1 - row2)

# 6. list and plot all the data for probe "Ta.10026.1.A1_at"
data <- plex_dat[row.names(plex_dat) == "Ta.10026.1.A1_at",]
plot(t(plex_dat[row.names(plex_dat) == "Ta.10026.1.A1_at",]))
lines(t(plex_dat[row.names(plex_dat) == "Ta.10026.1.A1_at",]))

# 7. create a histogram of the data in column "X2.WL711.susceptible.irrigated"
data <- plex_dat$"X2.WL711.susceptible.irrigated"
hist(plex_dat$"X2.WL711.susceptible.irrigated")

# 8. Plot the data in row 5
plot(t(plex_dat[5,]))
lines(t(plex_dat[5,]))
@
\fi

\subsection{The data.frame}
We've now read some data into a variable. The type of this variable is a \textit{data.frame}. \\
\\
data.frames are great. They're the most common way of manipulating data in R. They are ideal for storing and manipulating experimental data. Think of them as similar to a spreadsheet, where each column can contain one type of data. Data can be sliced and diced in many ways, and it is easy to aggregate the data too.

Below, I create a data frame from three vectors, and play with different ways of addressing it. As you follow through, try the commands on the data.frame you have created from the plexDB data in the previous section.

<<data_frame, eval=TRUE, fig.width=3, fig.height=3, cache=FALSE, result='asis',background='#FDFDFD'>>=
## create three vectors.

# a character vector
cultivar <- c("RAC","RAC","RAC","EXCAL","EXCAL","EXCAL","EXCAL")
# a numeric vector
height   <- c( 0.6 ,0.57 ,0.62 , 0.92  , 0.77  , 0.55  , 0.76  )
# a boolean vector
drought  <- c(FALSE,TRUE, TRUE , FALSE , TRUE  , TRUE  , FALSE )

## combine the three vectors into a data.frame
df <- data.frame(cultivar, height,drought)
# look at the data frame
df
# get the column names of the data.frame
names(df)
# get the row names of the data.frame
row.names(df)
# get the dimensions of the data frame
dim(df)
# get the number of rows
nrow(df)
# get the number of columns
ncol(df)

## explore various ways of subsetting the data.frame

# by address (get rows 1 and 2, and columns 1 to 3)
df[1:2,1:3]

# get all the height data
df$height

# get everything except the third column
df[,-3]
df[,c("cultivar", "height")]
df[,1:2]

# get the drought data only and drop the third column
df[df$drought,-3]

# visualise the data
library(ggplot2)
# add a new column for better annotation of the figure
df$treatment[df_plot$drought] <- "drought"
df$treatment[!df_plot$drought] <- "control"
ggplot(df, aes(cultivar, height)) + geom_point() + facet_wrap(~treatment)
@

%%% PROBLEMS
\subsubsection{Exercises}
<<df_work1, eval=TRUE, cache=FALSE, result='asis', size="large",background='#D7FFD7', highlight=FALSE>>=
# 1.1. there is a data.frame called mtcars -- look at it
# Note: if you break the mtcars data somehow, you can reload/reset it by typing
data(mtcars)

# 1.2. Extract the second row of mtcars, and store it in a vector

# 1.3. Extract the second through fifth rows of mtcars, and store them in a data.frame

# 1.4. create a table counting the number of cars with different number of cylinders (hint: use the "table" function)

# 1.5.a extract the column cyl from mtcars

# 1.5.b create a new boolean vector that is TRUE where the value in cyl is 4, and is FALSE otherwise

# 1.5.c use this boolean vector to extract the rows of mtcars where the number of cylanders is 4

# 1.5.d for these cars, calculate the mean value of hp (hint: use the [mean] function)

# 1.5.e repeat this for 6 and 8 cylander cars (try to find ways to combine these steps)

# 1.6. for each car, calculate the hp to cyl ratio (horse-power per cylinder: a very important quantity!), store it in a new column in the dataframe called "hp2cyl"

# 1.7. It has been found that the cars' weight was overestimated by 5% for cars weighing more than 3.5 tons -- correct this

## look at some plexDB data

# 2.1. in the previous exercises, we read in a file called plex_dat_small.csv -- this is a small version of the file plex_dat.csv -- this time, read in the whole file, and store it in a data.frame

# 2.2. how many rows and columns are there in this file?

# 2.3. what are the column names?

# 2.3. plot a histogram of column X3.WL711.susceptible.irrigated (hint: use the "hist" function)

# 2.4. plot a histogram of row Ta.10015.3.S1_at

@

<<df_work2, eval=TRUE, cache=FALSE, result='asis', size="large",background='#FfEDD9', highlight=FALSE>>= 
# 1.8. from the mtcars data.frame, create a scatterplot of mpg vs hp (hint: use the "plot" function)

# 2.5 Create a scatterplot of probe Ta.8722.1.S1_at versus probe Ta.11279.1.A1_at

# 2.6 Calculate the Pearson correlation between probes Ta.8722.1.S1_at and Ta.11279.1.A1_at 

@

<<df_work3, eval=TRUE, cache=FALSE, result='asis', size="large",background='#FFDDDD', highlight=FALSE>>=
# 1.9. install the "plyr" library, and repeat what you did in exercise 1.5 using the function ddply, a function in that library

# 1.10. install and load the "ggplot2" library. Then explore different visualisations of the mtcars data
@

%%% SOLUTIONS
\iftrainermanual
\subsubsection{Trainer solutions}
<<df_soln, eval=FALSE, cache=FALSE, result='asis', size="large",background='#DDFFFF', highlight=FALSE>>=
# 1.1. there is a data.frame called mtcars -- look at it
# Note: if you break the mtcars data somehow, you can reload/reset it by typing
data(mtcars)
mtcars

# 1.2. Extract the second row of mtcars, and store it in a vector
mtcars[2,]

# 1.3. Extract the second through fifth rows of mtcars, and store them in a data.frame
mtcars[2:5,]

# 1.4. create a table counting the number of cars with different number of cylinders (hint: use the "table" function)
table(mtcars$cyl)

# 1.5.a extract the column cyl from mtcars
mtcars$cyl
# 1.5.b create a new boolean vector that is TRUE where the value in cyl is 4, and is FALSE otherwise
boolVec <- mtcars$cyl == 4
# 1.5.c use this boolean vector to extract the rows of mtcars where the number of cylanders is 4
FourCylCars <- mtcars[boolVec,]
# 1.5.d for these cars, calculate the mean value of hp (hint: use the [mean] function)
mean(FourCylCars$hp)
# 1.5.e repeat this for 6 and 8 cylander cars (try to find ways to combine these steps)
mean(mtcars[mtcars$cyl==4,]$hp)
mean(mtcars$hp[mtcars[,"cyl"]==6])
mean(mtcars[mtcars[,2]==8,"hp"])

# 1.6. for each car, calculate the hp to cyl ratio (horse-power per cylinder: a very important quantity!), store it in a new column in the dataframe called "hp2cyl"

mtcars$hp2cyl <- mtcars$hp / mtcars$cyl

# 1.7. It has been found that the cars' weight was overestimated by 5% for cars weighing more than 3.5 tons -- correct this# 1.1. there is a data.frame called mtcars -- look at it
mtcars$wt[overweight_cars] <- mtcars$wt[overweight_cars] / 1.05

### look at some plexDB data
### ########################

# 2.1. in the previous exercises, we read in a file called plex_dat_small.csv -- this is a small version of the file plex_dat.csv -- this time, read in the whole file, and store it in a data.frame
plex_dat <- read.table("plex_dat.dat", header=TRUE, quote="\"")

# 2.2. how many rows and columns are there in this file?
dim(plex_dat)

# 2.3. what are the column names?
names(plex_dat)

# 2.3. plot a histogram of column X3.WL711.susceptible.irrigated (hint: use the "hist" function)
hist(plex_dat[,"X3.WL711.susceptible.irrigated"])

# 2.4. plot a histogram of row Ta.10015.3.S1_at
hist(plex_dat["Ta.10015.3.S1_at",])

# 1.8. from the mtcars data.frame, create a scatterplot of mpg vs hp (hint: use the "plot" function)
plot(mtcars$mpg, mtcars$hp)

# 2.5 Create a scatterplot of probe Ta.8722.1.S1_at versus probe Ta.11279.1.A1_at
plot(plex_dat["Ta.8722.1.S1_at",],plex_dat["Ta.11279.1.A1_at",])

# 2.6 Calculate the Pearson correlation between probes Ta.8722.1.S1_at and Ta.11279.1.A1_at 
cor(plex_dat["Ta.8722.1.S1_at",],plex_dat["Ta.11279.1.A1_at",])

# 8. install the "plyr" library, and repeat what you did in step 4 using the function ddply, a function in that library
install.packages("plyr")
library(plyr)
ddply(mtcars, .(cyl), function(x) mean(x$hp))

# 1.10. install and load the "ggplot2" library. Then explore different visualisations of the mtcars data
p <- ggplot(mtcars, aes(wt, mpg))
p <- p + geom_point(aes(size = qsec, colour=hp, shape=factor(cyl))) +
         scale_colour_gradient(low = "blue", high="red") +
         ggtitle("a 5-dimensional graph")
print(p)

@
\fi

\section{Wrap-up Part I}
In this section, we have covered
\begin{itemize}
\item{Basic data types in R: vector, data.frame}
\item{Reading in data from a csv file}
\item{Using libraries and help}
\item{Subsetting a data.frame}
\end{itemize}

\part{R/qtl and R/ASMap}

In this section we will introduce two R packages that can be used
simultaneously to perform very fast construction and diagnosis of
genetic linkage maps for inbred populations such as backcrosses (BC),
doubled haploids (DH) and recombinant inbreds (RI). The packages are
\begin{itemize}
  \item \textbf{R/qtl}: This is a map construction and QTL analysis
    package written by Karl Broman. The package handles a myriad of
    genetic populations and is very useful for manipulation,
    exploration and tabulation of your genetic marker set. It can also
    perform QTL analysis which will not be discussed in this workshop.
  \item \textbf{R/ASMap}: (Formerly known as R/qtlPlus). This new
    package written by Julian Taylor
    unlocks the lightning quick MSTmap algorithm of Wu et al (2008)
    for linkage map construction of inbred populations. It also
    includes graphical diagnostic functions for rapid assessment of
    linkage map problems. All functions use an R/qtl format for the
    data object so they can also be manipulated with functions from the R/qtl package.
\end{itemize}
<<qtl_01, eval=TRUE, include = FALSE>>=
library(qtl)
library(ASMap)
@
To install R/qtl and R/ASMap, type \texttt{}
<<qtl_02, eval=FALSE, cache=FALSE, result='asis',warning=FALSE>>=
# load the libraries
install.packages('qtl')
install.packages('ASMap')
library(qtl)
library(ASMap)
@

\section{Preparing data for R/qtl}

In this workshop we will be using two datasets available in the ASMap
package. One is a constructed linkage map of a DH population that
has markers clustered and optimally ordered in assigned chromosomes
(exmapC.csv) and the other is an unconstructed version of the same map
with all markers assigned to one chromosome (exmapU.csv). In the
latter map I have also added other markers with potential problems
that we may need to take care of before constructing the map.

The R/qtl package includes a function, \texttt{read.cross()} that is
designed to read a specifically formatted file containing genetic marker
data. It can handle a large range of formats including files exported
from MapManager, MapMaker and QTL Cartographer. It also handles
Micorsoft Excel files in CSV format and ``rotated'' CSV format (see
\texttt{?read.cross} for more details). The format we will be using
in this workshop will be the roatated CSV format. Note, this is not a format
that actually exists in Excel. It is just a type of CSV file that Karl
Broman has invented for use with the R/qtl package.

The first 10 columns and 15 rows of the rotated CSV file (exampC.csv) we will be
reading in is given in the table \ref{fig:spread}. Important things to note about this format
\begin{enumerate}
  \item Marker names are always in \textbf{first} column
  \item Linkage group names are always in \textbf{second} column
  \item Genetic distances are always in \textbf{third} column
    (optional)
  \item Phenotype columns are the rows above the marker
    information. This includes the line identifier column for the
    genotypes
  \item The blank cells need to remain blank.
\end{enumerate}
This is a very specific format and it has to be accurate to ensure the
data is accurately read into R/qtl. This may appear like a lot of work
but once you have set this up once or twice it becomes second nature.

\begin{figure}[t]
 \centering
\includegraphics[width = 13cm]{spreadsheet}
\caption{Subset of the rotated CSV format of exmapC.csv}
 \label{fig:spread}
\end{figure}

\section{Reading data into R/qtl}

An example of a typical call to \texttt{read.cross} would look like
<<qtl-0,eval=FALSE,echo=TRUE>>=
exmap <- read.cross(file = "exmapC.csv", format = "csvr", genotypes = c("AA", "BB"), na.strings = c("-"), map.function = "kosambi", estimate.map = FALSE)
@
Each of the arguments plays an important role in determining how the
data is read in
\begin{enumerate}
\item{\texttt{file = "exmapC.csv"}: which file to load}
\item{\texttt{format = "csvr"}: format of the file}
\item{\texttt{genotypes = c("AA", "BB")}: the way the alleles are
    coded in the file}
\item{\texttt{na.strings = c("-")}: how missing data are coded in
  the file}
\item{\texttt{map.function = "kosambi"}: the function to be used for
    genetic distance between markers.}
\item{\texttt{estimate.map = FALSE}: whether to estimate the genetic
  distances (constructed maps only!)}
\end{enumerate}
More information can be found by typing \texttt{?read.cross} in
R.

It is important to understand the structure of the data once it is
read in as this will allow you to explore and manipulate parts of the
R/qtl object. The explicit structure of the object can be summarised
graphically in \ref{fig:struc} and can be viewed as a hierarchical
list. Any box below another box with arrow connecting is a list
element of the box above it. The terminal boxes at the bottom are
where the data is placed once the object is read in.

For example, following the graphic down the left hand side of the tree
you will see the \texttt{object} has a \texttt{pheno} list element. This is
where the phenoptypic components of the data are placed. They are
stored in a data frame with $s$ columns named by \texttt{Y1}, \texttt{Y2},
$\ldots$, \texttt{Ys}. These names are taken directly from the
rotated CSV file. \textbf{Note: The column that contains the genotype names is also stored here.}

Following the graphic down the right hand side of the tree there is a
\texttt{geno} list element that contains all the genetic marker
information. The \texttt{geno} element contains list elements named by the
$n$ linkage group names \texttt{L1}, \texttt{L2}, $\ldots$,
\texttt{Ln}. Each of these elements contains two list elements
\texttt{map} and \texttt{data}. The \texttt{map} contains a vector of
genetic distances for the linkage group named by the marker names
and the \texttt{data} component contains the allelic information
for the linkage group in the form of a matrix with each marker
representing a column. Both \texttt{map} and \texttt{data} elements
are stored in the same order as they are read from the rotated CSV.

This structure must remain intact throughout any manipulation of the
object. R/qtl contains some error trapping to check this but
learning what the structure of an R/qtl object is the best prevention
of problems.

\begin{figure}[t]
 \centering
\includegraphics[width = 15cm]{structureg}
\caption{Structure of an R/qtl object.}
 \label{fig:struc}
\end{figure}

\section{Exploring the R/qtl object}

In this section we will read in the constructed linkage map into R/qtl
explore the resulting object by looking at the
hierarchical components described in Figure \ref{fig:struc}.

<<qtl_1, eval=TRUE, cache=FALSE, result='asis',warning=FALSE>>=

# the same data can be read in from a file using read.cross()
exmapc <- read.cross(file = "exmapC.csv", format = "csvr", genotypes = c("AA", "BB"),
         na.strings = "-", map.function = "kosambi", estimate.map = FALSE)

# structure of object is a list with two elements "pheno and "geno"
names(exmapc)

# The "pheno" element is a data frame of phenotypes including a column of genotype names
# The "Genotype" column in the data frame SHOULD be a factor with levels equivalent to the line names
str(exmapc$pheno)

# The geno element contains the linkage groups
names(exmapc$geno)
str(exmapc$geno$"1A")

# Each linkage group has two elements, "map" and "data".For each linkage group,
# the length of the "map" and the number of columns of the "data" will match each
# other. The rows exactly line up with the names in exmap$pheno$Genotype. R/qtl
# DOES NOT put row names on the data. By default, R/qtl sets the allele codes as
# AA = 1, BB = 2 for a DH and AA = 1, AB = 2 for a BC.
exmapc$geno$"1A"$map
dim(exmapc$geno$"1A"$data)

# R/qtl provides some quick summary functions for components of the object
# Summarize the object.
summary(exmapc)

# total number of markers
# number of markers on linkage groups
# number of linkage groups
# number of individuals
totmar(exmapc)
nmar(exmapc)
nchr(exmapc)
nind(exmapc)

# The object also has a class structure
class(exmapc)

# the "bc" component of this cross represents a "backcross" so R/qtl knows this is a simple crossing
# of two parents (R/qtl handles more difficult crosses that are not discussed here).
@

%%% PROBLEMS
\subsection{Exercises}
<<qtl_1_work1, eval=TRUE, cache=FALSE, result='asis', size="large",background='#D7FFD7', highlight=FALSE>>=
# 1. Install and load the required function libraries

# 2. Read in exmapC.csv using read.cross

# 3. How many linkage group does the map have?

# 4. How many markers does the linkage map have?

# 5. What is the length of the "3B" linkage group?

# 6. What is the first row of allele data in the "3B" linkage group?
@
<<qtl_1_work2, eval=TRUE, cache=FALSE, result='asis', size="large",background='#FfEDD9', highlight=FALSE>>=
# 7. What is the name of the last individual in the linkage map?
@

<<qtl_1_work3, eval=TRUE, cache=FALSE, result='asis', size="large",background='#FFDDDD', highlight=FALSE>>=
# 8. Extract the lengths of each of the linkage groups in a vector. (Hint: use sapply())

# 9. Extract the name of the first marker in each linkage group in a vector. (Hint: use sapply())

# 10. Extract the seventh and eight marker from linkage group 1B?

# 11. How many recombinations have occured between these markers?

# 12. How many missing values does each marker have in the "7A" linkage group?
@
%%% SOLUTIONS
\iftrainermanual
\subsection{Trainer solutions}
<<qtl_1_soln, eval=FALSE, cache=FALSE, result='asis', size="large",background='#DDFFFF', highlight=FALSE>>=
# 1. Install and load the required function libraries
install.packages("qtl")
install.packages("ASMap")
library(qtl)
library(ASMap)

# 2. read in exmapC.csv using read.cross
exmapc <- read.cross(file = "exmapC.csv", format = "csvr", genotypes = c("AA", "BB"),
         na.strings = c("-"), map.function = "kosambi", estimate.map = FALSE)

# 3. How many linkage groups does the map have?
length(nmar(exmapc))
length(exmapc$geno)

# 4. How many markers does the linkage map have?
sum(nmar(exmapc))
totmar(exmapc)

# 5. What is the length of the "3B" linkage group?
length(exmapc$geno$"3B"$map)

# 6. What is the first row of allele data in the "3B" linkage group?
exmapc$geno$"3B"$data[1,]

# 7. What is the name of the last individual in the linkage map?
exmapc$pheno$Genotype[length(exmapc$pheno$Genotype)]

# 8. Extract the lengths of each of the linkage groups in a vector (Hint: use sapply().)
sapply(exmapc$geno, function(x) max(x$map))

# 9. Extract the name of the first marker in each linkage group in a vector (Hint: use sapply().)
sapply(exmapc$geno, function(x) names(x$map[1]))

# 10. Extract the seventh and eight marker from linkage group 1B?
mk <- exmapc$geno$"1B"$data[,7:8]

# 11. How many recombinations have occured between these markers? (Hint: use apply().)
sum(abs(apply(mk, 1, diff), na.rm = TRUE))

# 12. How many missing values does each marker have in the "7A" linkage group? (Hint: use apply().)
apply(exmapc$geno$"7A"$data, 2, function(x) length(x[is.na(x)]))
@

\section{Map interrogation and modification tools}

R/qtl and R/ASmap provide users with a suite of functions to
interrogate and modify their linkage maps.

<<qtl_2, eval=TRUE, cache=FALSE, result='asis',warning=FALSE>>=

######### Subsetting by linkage groups (i.e. removing chromosomes from linkage map)

# this can be done in multiple different ways

# 1. By explicitly naming the linkage groups
exmaps <- subset(exmapc, chr = c("1A","1B1","1B2"))

# 2. By using the names of the "geno" part of the object
exmaps <- subset(exmapc, chr = names(exmapc$geno)[1:3])

# 3. By setting up a logical vector
wchr <- names(exmapc$geno) %in% names(exmapc$geno)[1:3]
exmaps <- subset(exmapc, chr = wchr)
nmar(exmaps)

######### Subsetting individuals (i.e. removing genotypes from linkage map)

# removing first three genotypes, this can be done multiple ways

# 1. Stating numerically which ones to drop
exmaps <- subset(exmapc, ind = -(1:3))

# 2. Stating logically which ones to drop

whind <- !(exmapc$pheno$Genotype %in% exmapc$pheno$Genotype[1:3])
exmaps <- subset(exmapc, ind = whind)
nind(exmaps)

# pull "map" elements of linkage groups
pull.map(exmapc)[["1B1"]]

# marker names of the complete linkage map
markernames(exmapc)[1:10]

# drop markers and return object
exmapd <- drop.markers(exmapc, c("1B1.m.1","1B1.m.2"))
pull.map(exmapd)[["1B1"]]

# move marker from what linkage group to another and return object
# (places marker at end of linkage group).
# this moves marker "1A.m1.1" to the end of linkage group "1B".
exmapm <- movemarker(exmapc, "1A.m.1","1B1")
pull.map(exmapm)[["1B1"]]

# this switches the numeric values of the marker "1A.m.1"
# (i.e. 1 becomes 2 and 2 becomes 1)
exmapm <- switchAlleles(exmapc, "1A.m.1")

# returns chromosome and cm position of markers on linkage map
find.markerpos(exmapc, c("1A.m.1","2A.m.7"))

# breaking linkage groups (ASMap function, see ?breakCross)
exmapb <- breakCross(exmapc, split = list("1A" = "1A.m.4"))
nmar(exmaps)

# merging linkage groups (ASMap function, see ?mergeCross)
exmapm <- mergeCross(exmapb, merge = list("2D" = c("2D1","2D2")))
nmar(exmapm)

# order linkage groups (uses mixedorder() from library gtools)
exmapc$geno <- exmapc$geno[mixedorder(names(exmapc$geno))]
@

%%% PROBLEMS
\subsection{Exercises}
<<qtl_2_work1, eval=TRUE, cache=FALSE, result='asis', size="large",background='#D7FFD7', highlight=FALSE>>=

# 1. Create a new map that contains only marker data for the 7* chromosomes.

# 2. Create a new map that contains the first 100 individuals only

# 3. Extract all the 3* linkage group map elements

# 4. Drop all the markers in 1B1 and extract the linkage group information. What do you notice?

# 5. Move a marker from 7D to a new linkage group 7D1. What do you notice?
@

<<qtl_2_work2, eval=TRUE, cache=FALSE, result='asis', size="large",background='#FfEDD9', highlight=FALSE>>=

# 6. Subset the linkage map by removing linkage groups with less than 10 markers.

# 7. Extract all the map elements for the linkage groups with less than 10 markers

# 8. Merge the 1B1 amd 1B2 linkage groups together.

# 9. Break the linkage group 3A at the 20th marker.
@

%%% SOLUTIONS
\iftrainermanual
\subsection{Trainer solutions}
<<qtl_2_soln, eval=FALSE, cache=FALSE, result='asis', size="large",background='#DDFFFF', highlight=FALSE>>=

# 1. Create a new map that contains only marker data for the 7* chromosomes.
exmaps <- subset(exmapc, chr = c("7A","7B","7D"))

# 2. Create a new map that contains the first 100 individuals only
exmaps <- subset(exmapc, ind = 1:100)

# 3. Extract all the 3* linkage group map elements from "exmapc"
pull.map(exmapc)[c("3A","3B","3D")]

# 4. Drop all the markers in 1B1 and extract the linkage group information. What do you notice?
exmapd <- drop.markers(exmapc, names(pull.map(exmapc)[["1B1"]]))
nmar(exmapd)

# The 1B1 linkage group has disappeared from the new object.

# 5. Move a marker from 7D to a new linkage group 7D1. What do you notice?
exmapm <- movemarker(exmapc, "7D.m.1", "7D1")

# The 7D1 linkage group is created when you move a marker to it.

# 6. Subset the linkage map by removing linkage groups with less than 10 markers
exmaps <- subset(exmapc, chr = names(exmapc$geno)[nmar(exmapc) > 9])

# 7. Extract all the map elements for the linkage groups with less than 10 markers
exmaps <- subset(exmapc, chr = names(exmapc$geno)[nmar(exmapc) < 10])
pull.map(exmaps)

# 8. Merge the 1B1 amd 1B2 linkage groups together.
exmapm <- mergeCross(exmapc, merge = list("1B" = c("1B1","1B2")))
nmar(exmapm)
pull.map(exmapm)[["1B"]]

# 9. Break the linkage group 3A at the 20th marker.
exmapb <- breakCross(exmapc, split = list("3A" = "3A.m.20"), suffix = list("3A" = c("3A1","3A2")))
nmar(exmapb)
pull.map(exmapb)[c("3A1","3A2")]

@

\section{Pre-map construction diagnostics}
\label{sec:pre}

One of the most important aspects of linkage map construction is
being able to effciently assess and diagnose potential problems
associated with your marker data. Some problems can be identified before the
linkage map is constructed and some may require checking after construction is
complete.

In the age of high-throughput technology it would seem wise to
construct your map from the best set of markers you have avalaible. To
ensure this a basic pre-construction checklist could be
\begin{enumerate}
  \item Check for monomorphic markers and remove. These markers
    indicate a genomic region that does not segregate for the
    population being studied.
  \item Check for markers with large numbers of missing values and
    remove. These markers will struggle to map to unique locations on the
    linkage map.
  \item Check for significant segregation distortion in the markers
    and remove. Distorted markers will also struggle to map to unique
    locations.
  \item Place co-locating markers aside.
\end{enumerate}

<<qtl_3, eval=TRUE, cache=FALSE, result='asis',out.width ="15.5cm",out.height="11cm",fig.width = 12,fig.height = 9,fig.align = "center",warning=FALSE>>=

# read in unconstructed map
exmapu <- read.cross(file = "exmapU.csv", format = "csvr", genotypes = c("AA", "BB"),
         na.strings = c("-"), map.function = "kosambi", estimate.map = FALSE)

# plot missing values across complete linkage map
plot.missing(exmapu)

# R/ASMap contains a profiling function for the markers that can profile
# multiple variables at once. Here is a profile of the segregation distortion,
# allele proportions and missing value proportions of each marker.
profileMark(exmapu, stat.type = c("seg.dist","prop","miss"), layout = c(1,4), type = "l", crit.val = "bonf")

# The plot annotates markers that have significant segregation distortion and also
# highlights markers with large amounts of miissing values. You can remove them by dropping
# the markers. First calculate statistics for each marker.
sm <- statMark(exmapu, stat.type = "marker")

# drop missing
miss.markers <- markernames(exmapu)[sm$marker$missing > 0.4]
miss.markers
exmaps <- drop.markers(exmapu, miss.markers)

# drop distorted markers
crit.val <- -log10(0.05/totmar(exmapu))
dist.markers <- markernames(exmapu)[sm$marker$neglog10P > crit.val]
dist.markers
exmaps <- drop.markers(exmaps, dist.markers)

# plot again (looking good!)
profileMark(exmaps, stat.type = c("seg.dist","prop","miss"), layout = c(1,4), type = "l", crit.val = "bonf")

# R/ASMap has a function to pull co-located markers from a map,
# keeping their connections (see ?pullCross)
exmapu.p <- pullCross(exmaps, type = "co.located")
exmapu.p$co.located$table
@

<<qtl_3_work1, eval=TRUE, cache=FALSE, result='asis', size="large",background='#D7FFD7', highlight=FALSE>>=

# 1. Read in the unconstructed map and follow the commands from this section.
@

\iftrainermanual
\subsection{Trainer solutions}
<<qtl_3_soln, eval=FALSE, cache=FALSE, result='asis', size="large",background='#DDFFFF', highlight=FALSE>>=

# 1. Read in unconstructed map and follow the commands from this section.
exmapu <- read.cross(file = "exmapU.csv", format = "csvr", genotypes = c("AA", "BB"),
         na.strings = c("-"), map.function = "kosambi", estimate.map = FALSE)
plot.missing(exmapu)
profileMark(exmapu, stat.type = c("seg.dist","prop","miss"), layout = c(1,4), type = "l", crit.val = "bonf")
sm <- statMark(exmapu, stat.type = "marker")
miss.markers <- markernames(exmapu)[sm$marker$missing > 0.4]
miss.markers
exmaps <- drop.markers(exmapu, miss.markers)
crit.val <- -log10(0.05/totmar(exmapu))
dist.markers <- markernames(exmapu)[sm$marker$neglog10P > crit.val]
dist.markers
exmaps <- drop.markers(exmaps, dist.markers)
exmapu <- pullCross(exmaps, type = "co.located")
exmapu$co.located$table
@

\section{Map construction using R/ASMap}

The R/qtl package contains a suite of functions that can be used for
linkage map construction. These include the linkage group clustering
function \texttt{formLinkageGroups()} and optimal marker ordering
functions \texttt{orderMarkers()}, \texttt{switchOrder()} and
\texttt{ripple()}. Unfortunately, these marker ordering functions use a
sliding window or rippling method for determining optimal marker
position. These methods are disadvantageous for several
reasons. Firstly, markers can only be moved within the sliding
window. This window becomes limited when the number of markers in the
linkage group is increased. As an example, using a sliding window of 8
marker positions in a linkage group with 50 markers becomes
computationally intensive. In addition, markers can only move a
maximum of 8 positions from their original position. This suggests that
rippling may need to be performed several times to ensure an optimal
order of the markers is found for the linkage group. Users can check this for themselves by
using the function \texttt{ripple()} with window 8 on the linkage group 5A of the
constructed map ``exmap''.

R/ASMap was written to completely circumvent the computational issues described
above. The package contains two extremely efficient linkage map construction functions,
\texttt{mstmap.data.frame()} and \texttt{mstmap.cross()}, that unlock the
MSTmap source code (a full donwload of the source code and more
infomation on MSTmap can be found at
\url{http://alumni.cs.ucr.edu/~yonghui/mstmap.html}) and make it
available in R. In this workshop we will focus on
\texttt{mstmap.cross()} which contains a comprehensive set of
arguments that allow users to completely construct, re-construct an
R/qtl cross object. A more detailed description of the arguments
can be found by accessing the help file \texttt{?mstmap.cross}.

<<qtl_4, eval=TRUE, echo=TRUE, cache=FALSE, result='asis'>>=
args(mstmap.cross)
@
A short description of the most conmmonly required arguments is given
below.
\begin{enumerate}
\item{\texttt{object}: the R/qtl cross object.}
\item{\texttt{chr}: the names of the linkage groups to use when
    constructing the linkage map.}
\item{\texttt{id}: the column identifier for the genotype names in the
    \texttt{pheno} element of the R/qtl object.}
\item{\texttt{bychr}: logical value determining whether constrcution
    should occur within linkage groups}
\item{\texttt{anchor}: logical value determining whether linkage
    group orientations should be attempted to be preserved.}
\item{\texttt{dist.function}: distance function to be used in
    the calculation of the genetic distances}
\item{\texttt{p.value}: the threshold for determining linkage between
    distinct groups}
\item{\texttt{mv.est}: logial value determining whether missing values
  are imputed as the algorithm proceeds}
\item{\texttt{detectBadData}: logical value determining whether
    genoptyping errors are detected and set to missing as the
    algorithm proceeds}
\end{enumerate}

The function is higly flexible and allows you to construct or
reconstruct your object in almost every conceivable way. One of the
most important arguments of the function is the numerical
\texttt{p.value} that is used to determine the threshold for markers
clustering into linkage groups. The authors of MSTmap recommend that a
0.00001 or a LOD score of 5 (LOD score = log10(- p.value))
is a good initial \texttt{p.value} for most populations. However,
this value actually depends on the size of the population and how
strict you want your threshold to be. Figure \ref{fig:pval} shows the
relationship of LOD score against the number of genotypes in the
population for different thresholds of genetic distance between
linkage group clusters. For example, if you have a population with 300
lines and you were placing a threshold of 30 cM of distance between
linkage group clusters then an appropriate LOD score would be 14 or a
\texttt{p.value} = $10^{-14}$.

\begin{figure}[t]
 \centering
\includegraphics[width = 12cm]{pval}
\caption{LOD score of linkage vs number of genotypes for different
  genetic distance thresholds.}
 \label{fig:pval}
\end{figure}


<<qtl_41, eval=FALSE, echo=TRUE, cache=FALSE, result='asis'>>=

# Reconstruct the object from scratch! Setting bychr = FALSE bulks already
# formed linkage groups together and reconstructs.
map1 <- mstmap(exmapc, bychr = FALSE, dist.fun = "kosambi", trace = FALSE)
nmar(map1)

# Reconstruct the first three linkage groups.
map2 <- mstmap(exmapc, chr = names(exmap$geno)[1:3], bychr = FALSE, dist.fun = "kosambi", trace = FALSE)
nmar(map2)

# Reconstruct individual linkage groups, breaking linkage groups if necessary
map3 <- mstmap(exmapc, bychr = TRUE, dist.fun = "kosambi", trace = FALSE)
nmar(map3)

# Optimally order markers within linkage groups without breaking linkage groups.
map4 <- mstmap(exmapc, bychr = TRUE, dist.fun = "kosambi", trace = FALSE, p.value = 2)
nmar(map4)
pull.map(map4)[["1B1"]]

# Optimally order markers within linkage groups without breaking linkage groups and
# attempt to preserve orientation of linkage groups
map5 <- mstmap(exmapc, bychr = TRUE, dist.fun = "kosambi", trace = FALSE, p.value = 2, anchor = TRUE)
pull.map(map5)[["1B1"]]
@


%%% PROBLEMS
\subsection{Exercises}
<<qtl_4_work1, eval=TRUE, cache=FALSE, result='asis', size="large",background='#D7FFD7', highlight=FALSE>>=

# 1. Construct the unconstructed map "exmapu" and extract the linkage group names.

# 2. Extract the map elements of the newly constructed map. What do you notice?

# 3. Re-construct the 7* linkage groups of the constructed "exmapc" and extract their map elements.

# 4. Re-construct the 7* linkage groups again and attempt to preserve marker order.

@

<<qtl_4_work2, eval=TRUE, cache=FALSE, result='asis', size="large",background='#FfEDD9', highlight=FALSE>>=

# 5. Construct the unconstructed map "exmapu" from scratch with a stricter threshold and extract
# the map elements. What do you notice?

@

%%% SOLUTIONS
\iftrainermanual
\subsection{Trainer solutions}
<<qtl_4_soln, eval=FALSE, cache=FALSE, result='asis', size="large",background='#DDFFFF', highlight=FALSE>>=

# 1. Construct the unconstructed map "exmapu" and extract the linkage group names.
map1u <- mstmap(exmapu, bychr = FALSE, dist.fun = "kosambi", trace = FALSE)
nmar(map1u)

# 2. Extract the map elements of the newly constructed map. What do you notice?
pull.map(map1u)

# The linkage group names are L.* and some linkage groups appear in order and some do not. This
# is because MSTmap does not know the orientation of the linkage group.

# 3. Re-construct the 7* linkage groups of the constructed "exmapc" and extract their map elements.
map1c <- mstmap(exmapc, chr = c("7A","7B","7D"), bychr = FALSE, dist.fun = "kosambi", trace = FALSE)
pull.map(map1c)

# 4. Re-construct the 7* linkage groups again and attempt to preserve marker order.
map1c <- mstmap(exmapc, chr = c("7A","7B","7D"), bychr = FALSE, dist.fun = "kosambi", trace = FALSE, anchor = TRUE)
pull.map(map1c)

# 5. Construct the unconstructed map "exmapu" from scratch with a stricter threshold and extract
# the map elements. What do you notice?
map1us <- mstmap(exmapu, bychr = FALSE, dist.fun = "kosambi", trace = FALSE, p.value = 1e-10)
pull.map(map1us)

# There are more linkage groups. This is because the threshold for linkage between markers is stricter.

@

\section{Constructed map diagnosis}

Post construction diagnosis of your linkage map is very
important. A short checklist of potential things to look for are
\begin{enumerate}
\item How many distinct linkage groups are present?
\item Do the length of any linkage groups exceed the theoretical
  threshold of 200 cM?
\item Do recombination counts of any genotypes across the linkage map
  significantly exceed the number of expected recombination counts for
  a genotype given the reference genome?
\item Are there any problematic markers or regions of the genome?
\end{enumerate}

All of these points are very important diagnostics for a constructed
map. Points 1 and 2 can be checked by extracting the number of
linkage groups and the map elements of the constructed object.

From experience, one of the best indicators of a constructed maps
quality is the extraction and plotting of the recombination counts for
each of the genotypes across the complete linkage map (point number 3). Large counts
compared to an expected count for a genotype given the reference genome indicate
problematic lines.  There are a myriad of reasons these might exist
including wrong genotyping calls, DNA contamination, unknown population
structure etc. An increased number of recombinations for a set
of lines can have a significant impact on the quality of the linkage
map and this is usually manifested in the increased length of the
linkage groups. Although it is most likely that deletion is the only method of
to ensure ongoing linkage map quality, the problematic lines should be
scrutinised and resolved before continuining with the map construction process.

Point number 4 is the most difficult map diagnostic. It should be
noted that problematic markers could influence recombination counts
and double recombinations of genotypes. For this reason, points 3 and
4 should be checked simultaneously before deletions are
performed.

<<qtl_5, eval=TRUE, cache=FALSE, result='asis',out.width ="15.5cm",out.height="11cm",fig.width = 12,fig.height = 9,fig.align = "center">>=
# R/ASMap contains a function for plotting recombination counts, double recombination counts
# and missing value counts of each of the genotypes for the constructed linkage map.
profileGen(exmapc, bychr = FALSE, stat.type = c("xo","dxo","miss"), id = "Genotype",
           lty = 2, pch = 16, layout = c(1,3))

# R/ASMap also has a heat map that shows you how well the markers have clustered and
# linked together and is great visual guide for problemtic regions of your linkage map.
# It plots the estimated pairwise recombination fraction between markers on the top triangle
# and the pairwise LOD scores for the test of no linkage between markers on the bottom triangle.
#(see ?heatMap for more details)
heatMap(exmapc, lmax = 30)

# Statistics associated with the markers or intervals of the constructed linkage map can
# also be profiled.
profileMark(exmapc, stat.type = c("erf","lod","recomb","dxo"), layout = c(1,4), type = "l")
@

\subsection{Exercises}
<<qtl_5_work1, eval=FALSE, cache=FALSE, result='asis', size="large",background='#D7FFD7', highlight=FALSE>>=

# 1. Redo the commands above but with the newly constructed map "map1u".

# 2. For this map, subset the heatMap so it only contains the first 6 linkage groups.

# 3. Profile the constructed map on just L.18. What is unusual about these profiles?

@

<<qtl_5_work2, eval=TRUE, cache=FALSE, result='asis', size="large",background='#FfEDD9', highlight=FALSE>>=

# 4. Remove the genotype that has an unusual high number of recombinations

# 5. Reconstruct the linkage map with this line removed.
@

\subsection{Trainer solutions}
<<qtl_5_soln, eval=FALSE, cache=FALSE, result='asis', size="large",background='#DDFFFF', highlight=FALSE>>=

# 1. Redo the commands above but with the newly constructed map "map1u".
profileGen(map1u, bychr = FALSE, stat.type = c("xo","dxo","miss"), id = "Genotype", layout = c(1,3))
profileGen(map1u, bychr = FALSE, stat.type = c("xo","dxo","miss"), id = "Genotype",
           lty = 2, pch = 16, layout = c(1,3))
heatMap(map1u, lmax = 30)
profileMark(map1u, stat.type = c("erf","lod","recomb","dxo"), layout = c(1,4), type = "l")
pull.map(map1u)

# 2. For this map, subset the heatMap so it only contains the first 6 linkage groups.
heatMap(map1u, chr = names(map1u)[1:6], lmax = 30)

# 3. Profile the constructed map on just L.18. What is unusual about these profiles?
profileMark(map1u, chr = "L.18", stat.type = c("erf","lod","recomb","dxo"), layout = c(1,4), type = "l")

# There are two markers with 4 double crossovers that occur where there are small numbers of recombination.
# This would indicate possible problematic markers.

# 4. Remove the genotype that has an unusual high number of recombinations
sg <- statGen(map1u, bychr = FALSE, stat.type = "xo")
map1us <- subset(map1u, ind = sg$xo < 45)

# 5. Reconstruct the linkage map with this line removed.
map1us <- mstmap(map1us, bychr = TRUE, dist.fun = "kosambi", trace = FALSE, anchor = TRUE)
@





\section{Appendix}
\subsection{Installing R and RStudio}
On MS Windows, there are two steps:
\begin{enumerate}
\item{download \textit{R} from \url{http://cran.r-project.org/bin/windows/base/} and install it. }
\item{download \textit{RStudio desktop} from \url{https://www.rstudio.com/ide/download/} and install it}
\end{enumerate}

\end{document}
