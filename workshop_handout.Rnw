\documentclass{article} %\documentclass[a4paper,12pt,twoside]{memoir}
%\usepackage{btp}
%
% Example Workshop Handout with title page and a section demonstrating
% the different environments defined in the top matter


% if trainermanual is true, then include extra info in the report
\newif\iftrainermanual
\trainermanualtrue

\usepackage{url}
\usepackage[paper=a4paper,top=35mm,left=25mm,right=25mm,bottom=30mm]{geometry}

% Set some Workshop specific info
\iftrainermanual
\title{TRAINER MANUAL for R and rqtl workshop}
\else
\title{Introductory workshop for R and rqtl}
\fi
\date{May 2014}
\author{Angus Wallace and Julian Taylor}



\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
%\setWorkshopTitle{Introductory R workshop, rqtl}
%\setWorkshopVenue{ACPFG}
%\setWorkshopDate{May 2014}
%\setWorkshopAuthor{Angus Wallace}

%% this document can be rendered in R by typing:
%		CODEDIR <- Sys.getenv("CODEDIR")
%		DATADIR <- Sys.getenv("DATADIR")
%		setwd("~/R_workshop_handout/")
%		library(knitr)
%		Sys.setenv(TEXINPUTS=getwd(),
%				     BIBINPUTS=getwd(),
%				     BSTINPUTS=getwd())
%		knit2pdf("~/R_workshop_handout/workshop_handout.Rnw", output=paste(DATADIR,"/pdfs/R_workshop_handout.tex",sep=""))

\setlength{\parskip}{2ex}

<<setup, echo=FALSE, include=FALSE, cache=FALSE>>=
opts_chunk$set(tidy=TRUE, tidy.opts=list(width=75))
CODEDIR <- Sys.getenv("CODEDIR")
DATADIR <- Sys.getenv("DATADIR")
library(xtable)
library(reshape)
library(plyr)
library(stringr)
library(made4)
options(width=70)
@



%
% Workshop Title Page
%
%\workshoptitlepage

\section{This workshop}
\subsection{Aims}
Brief introduction to R\\
Basic use

\begin{enumerate}
\item{fundamental use}
\item{data types}
\item{read in data}
\item{quality-check and visualise data}
\item{write out data}
\item{rqtl}
\end{enumerate}
\section{How to read this document}

Each section in this workshop introduces a new concept or tool. Each section contains some examples and is followed by some problems for you to work through to cement your understanding. The facilitators are here to help you with these. It's hopefully clear that the purpose of these problems is to understand them, rather than get the "correct" pattern of letters and numbers on the screen!

\subsection{Code}
To use R, one must write code. This document contains code for different purposes, and it is formatted in a characteristic and consistent way throughout.
<<code_eg, eval=TRUE, cache=TRUE, result='asis',background='#FDFDFD'>>=
# boxes like this contain example code

# comments start with a hash 
# (like this line, which is not run)
print("this line has no hash at the start, so is run")

# Also, note that output generated by R has a double-comment

# There are three kinds of problems for you to attempt:
@

\subsubsection{Exercises (aka problems)}
<<ex_work1, eval=TRUE, cache=TRUE, result='asis', size="large",background='#D7FFD7', highlight=FALSE>>=
# You should try to complete all exercises in green
@

<<ex_work2, eval=TRUE, cache=TRUE, result='asis', size="large",background='#FfEDD9', highlight=FALSE>>= 
# If you are progressing well, attempt exercises in orange also (as an extension)
@

<<ex_work3, eval=TRUE, cache=TRUE, result='asis', size="large",background='#FFDDDD', highlight=FALSE>>=
# Purple exercises are designed to be difficult. If you can do these, great. Otherwise, you might want to think about them later.
@

\iftrainermanual
\subsubsection{Trainer solutions}
<<ex_soln, eval=TRUE, cache=TRUE, result='asis', size="large",background='#DDFFFF', highlight=FALSE>>=
# Trainer solutions to the Exercises are included on a blue background
@
\fi

\part{Working in R}
\section{Fundamentals}

@

\subsection{Functions}
Definition:
R uses \textit{functions} to get things done. Think of them as tools.
The functions are collated into \textit{libraries} (aka \textit{packages}) that can be loaded when needed. Think of them as toolboxes.
For example, in the same way that a carpenter may keep all their chisels together in a toolbox, a particular R library/package might contain functions to allow the plotting of data. \\
It is possible to write functions for commonly performed activities, and this is very useful (and acheivable!), but it is outside the scope of this workshop.

\subsection{Using libraries}
So, libraries/packages contain collections of functions, which are what get things done. When one first opens R, the \textit{base} library is automatically loaded, and provides the fundamental functions. \\
The following code shows the installation and loading of a library

<<libraries, eval=TRUE, cache=TRUE, result='asis',background='#FDFDFD'>>=
# install a package
install.packages("ggplot2")
# load the package
library(ggplot2)

# note that when installing, one needs to quote the package name, but when loading it one does not 
@

\subsection{Getting help}
You will do this constantly!
Most functions in R have a help page that explains how to use the function. One can access this by typing (eg.) \\
?ggplot\\

(a question mark, followed by the function name. Note that the function's library must be loaded first)

I generally use Google, but be warned -- because its name is so short, "R" is a difficult term to search for -- try to be specific

\section{What makes R useful? What can it do?}

\section{Data}

\subsection{load some data, from a file, into R}

We don't want to type in data by hand. Usually, we have data in a spreadsheet that we want to analyse. There are two main ways you can do this:
\begin{enumerate}
\item{If you have a spreadsheet program, you can select data, and copy it to the clipboard. Typing \\
\texttt{myData <- read.table(file="clipboard")} \\
will read those data into a variable called myData
}
\item{This is handy, but if this is something you will do again and again, it's a bit cumbersome. It's also easy to make mistakes. A more robust alternative is to export the data from the excel file to a csv\footnote{ Click on (File - Save As) and select csv or "Comma separated" as the type of file.}}
\end{enumerate}


The exact way to read in the data is determined by the format of your csv file. eg,

\texttt{\\
sample, cultivar, height, tillers, grain, group\\
1,      RAC,      3.25,   5,       4,     drought\\
2,      EXCAL,    3.25,   5,       4,     drought\\
3,      RAC,      3.25,   5,       4,     control\\
4,      EXCAL,    3.25,   5,       4,     control} \\
Then you would read it in with the command:
<<csv_read_1, eval=FALSE, cache=TRUE, result='asis',background='#FDFDFD'>>=
setwd("~/R_workshop_handout")  # set the working directory (where R looks for files)
filename     <- "plex_dat_small.csv"   # define the filename
drought_data <- read.table(filename, header=TRUE, sep=",", 
                            stringsAsFactors=FALSE)
@
\textit{(Note that factors are a type of data that we have not discussed and are out of scope for this workshop. Avoid them for now.)}

Alternatively, if the csv file looked like this:\\
\texttt{\\
"sample"  "cultivar"  "height"  "tillers"  "grain"  "group"\\
1       "RAC"       3.25    5        4      "drought"\\
2       "EXCAL"     3.25    5        4      "drought"\\
3       "RAC"       3.25    5        4      "control"\\
4       "EXCAL"     3.25    5        4      "control"\\
}\\
Then you would read it in with the command:
<<csv_2, eval=FALSE, cache=TRUE, result='asis',background='#FDFDFD'>>=
drought_data <- read.table(filename, header=TRUE, sep=" ", quote="\"", stringsAsFactors=FALSE)
@

The idea here, is that we have a quote symbol within quotes. In other words, we tell R which character denotes a quotation. Because the quotation character itself has special meaning to R, we have to tell R that we really mean the character and are not just finishing the quotation. We do this by putting a backslash in front of the character. This is also called "escaping" the character. If we just had three quote symbols in a row (without the backslash) R would think we were opening and closing a quotation, and then opening another -- the second one would never be closed, which would cause an error.

%%% PROBLEMS
\subsubsection{Exercises}

%% generate the csv file:
% plex_dat <- read.table("~/R_workshop_handout/TA44.dat", header=TRUE, quote="\"")
% plex_dat$affy <- gsub("_[0-9]{7}","",row.names(plex_dat))
% plex_dat <- plex_dat[!duplicated(plex_dat$affy),]
% row.names(plex_dat) <- plex_dat$affy
% plex_dat <- plex_dat[!duplicated(plex_dat$affy),]
% plex_dat <- plex_dat[,-25]
% write.table(plex_dat,file="~/R_workshop_handout/plex_dat.dat")

<<readin_work1, eval=TRUE, cache=TRUE, result='asis', size="large",background='#D7FFD7', highlight=FALSE>>=
# There is a csv file that you can read. It is located at [plex_dat_small.csv]. 

# 1. Open this file in RStudio, using [FILE -> Open File]
# in the menu. This will open the file in a text-editor within  
# RStudio so that its contents and formatting can be viewed.

# 2. Read in the file, using read.table, to a new variable.

# 3. What are the column names?

# 4. What are the first 10 row names?

# 5.1. Extract rows 1 and 2

# 5.2. Find the difference between the two rows

@

<<readin_work2, eval=FALSE, cache=TRUE, result='asis', size="large",background='#FfEDD9', highlight=FALSE>>= 

# 6. list and plot all the data for probe "Ta.10026.1.A1_at"

# 7. create a histogram of the data in column "X2.WL711.susceptible.irrigated" try: ??histogram
@

<<readin_work3, eval=TRUE, cache=TRUE, result='asis', size="large",background='#FFDDDD', highlight=FALSE>>=
# 8. Plot the data in row 5 (use the [plot] and [lines] functions)
@

%%% SOLUTIONS
\iftrainermanual
\subsubsection{Trainer solutions}
<<readin_soln, eval=FALSE, cache=TRUE, result='hide', size="large",background='#DDFFFF', highlight=FALSE>>=
# 1. 

# 2. Read in the file, using read.table, to a new variable.
plex_dat <- read.table("~/R_workshop_handout/plex_dat.dat", header=TRUE, quote="\"")

# 3. Make a list of the column names
names(plex_dat)

# 4. Make a list of the first 10 row names
row.names(plex_dat)[1:10]

# 5. Calculate the difference, at each sample, between rows 1 and 2
t(plex_dat[1,] - plex_dat[2,])

# 6. list and plot all the data for probe "Ta.10026.1.A1_at"
data <- plex_dat[row.names(plex_dat) == "Ta.10026.1.A1_at",]
plot(t(plex_dat[row.names(plex_dat) == "Ta.10026.1.A1_at",]))
lines(t(plex_dat[row.names(plex_dat) == "Ta.10026.1.A1_at",]))

# 7. create a histogram of the data in column "X2.WL711.susceptible.irrigated"
data <- plex_dat$"X2.WL711.susceptible.irrigated"
hist(plex_dat$"X2.WL711.susceptible.irrigated")

# 8. Plot the data in row 5
plot(t(plex_dat[5,]))
lines(t(plex_dat[5,]))
@
\fi

\subsection{The data.frame}
We've now read some data into a variable. The type of this variable is a \textit{data.frame}. \\
\\
data.frames are great. They're the most common way of manipulating data in R. They are ideal for storing and manipulating experimental data. Think of them as similar to a spreadsheet, where each column can contain one type of data. Data can be sliced and diced in many ways, and it is easy to aggregate the data too.

TODO: the data below are different -- rejig this section so that the data refer back to plexDat from the previous section.

<<data_frame, eval=TRUE, fig.width=3, fig.height=3, cache=TRUE, result='asis',background='#FDFDFD'>>=
cultivar <- c("RAC","RAC","RAC","EXCAL","EXCAL","EXCAL","EXCAL")
height   <- c( 0.6 ,0.57 ,0.62 , 0.92  , 0.77  , 0.55  , 0.76  )
drought  <- c(FALSE,TRUE, TRUE , FALSE , TRUE  , TRUE  , FALSE )
df <- data.frame(cultivar, height,drought)
df
names(df)
row.names(df)
dim(df)
nrow(df)
ncol(df)
# we've made a small data.frame with three columns and seven rows. How can we split the data?

# by address (get rows 1 and 2, and columns 1 to 3)
df[1:2,1:3]

# get all the height data
df$height

# get everything except the third column
df[,-3]
df[,c("cultivar", "height")]
df[,1:2]

# get the drought data only and drop the third column
df[df$drought,-3]

# visualise the data
library(ggplot2)
# add a new column for better annotation of the figure
df$treatment[df_plot$drought] <- "drought"
df$treatment[!df_plot$drought] <- "control"
ggplot(df, aes(cultivar, height)) + geom_point() + facet_wrap(~treatment)
@

%%% PROBLEMS
\subsubsection{Exercises}
<<df_work1, eval=TRUE, cache=TRUE, result='asis', size="large",background='#D7FFD7', highlight=FALSE>>=
# 1.1. there is a data.frame called mtcars -- look at it
# Note: if you break the mtcars data somehow, you can reload/reset it by typing
data(mtcars)

# 1.2. Extract the second row of mtcars, and store it in a vector

# 1.3. Extract the second through fifth rows of mtcars, and store them in a data.frame

# 1.4. create a table counting the number of cars with different number of cylinders (hint: use the "table" function)

# 1.5. calculate the mean hp of the 4, 6 and 8 cylinder cars separately (hint: use the "mean" function). Try to find at least three different ways of subsetting the data.frame

# 1.6. for each car, calculate the hp to cyl ratio (horse-power per cylinder: a very important quantity!), store it in a new column in the dataframe called "hp2cyl"

# 1.7. It has been found that the cars' weight was overestimated by 5% for cars weighing more than 3.5 tons -- correct this

## look at some plexDB data

# 2.1. in the previous exercises, we read in a file called plex_dat_small.csv -- this is a small version of the file plex_dat.csv -- this time, read in the whole file, and store it in a data.frame

# 2.2. how many rows and columns are there in this file?

# 2.3. what are the column names?

# 2.3. plot a histogram of column X3.WL711.susceptible.irrigated (hint: use the "hist" function)

# 2.4. plot a histogram of row Ta.10015.3.S1_at

@

<<df_work2, eval=TRUE, cache=TRUE, result='asis', size="large",background='#FfEDD9', highlight=FALSE>>= 
# 1.8. from the mtcars data.frame, create a scatterplot of mpg vs hp (hint: use the "plot" function)

# 2.5 Create a scatterplot of probe Ta.8722.1.S1_at versus probe Ta.11279.1.A1_at

# 2.6 Calculate the Pearson correlation between probes Ta.8722.1.S1_at and Ta.11279.1.A1_at 

@

<<df_work3, eval=TRUE, cache=TRUE, result='asis', size="large",background='#FFDDDD', highlight=FALSE>>=
# 1.9. install the "plyr" library, and repeat what you did in exercise 5 using the function ddply, a function in that library

# 1.10. install and load the "ggplot2" library. Then explore different visualisations of the mtcars data
@

%%% SOLUTIONS
\iftrainermanual
\subsubsection{Trainer solutions}
<<df_soln, eval=FALSE, cache=TRUE, result='asis', size="large",background='#DDFFFF', highlight=FALSE>>=
# 1. there is a data.frame called mtcars -- look at it
mtcars

# 2. create a table counting the number of cars with different number of cylinders (hint: use the "table" function)
table(mtcars$cyl)

# 3. plot a histogram of the milage of the cars (hint: use the "hist" function)
hist(mtcars$mpg)

# 4. calculate the mean hp of the 4, 6 and 8 cylinder cars separately (hint: use the "mean" function). Note the different ways of addressing the data frame -- this should be emphasised to course attendees
mean(mtcars[mtcars$cyl==4,]$hp)
mean(mtcars$hp[mtcars[,"cyl"]==6])
mean(mtcars[mtcars[,2]==8,"hp"])

# 5. create a scatterplot of mpg vs hp (hint: use the "plot" function)
plot(mtcars$mpg, mtcars$hp)

# 6. for each car, calculate the hp to cylinder ratio (horse-power per cylinder: a very important quantity!), store it in a new column in the dataframe called "hp2cyl"
mtcars$hp2cyl <- mtcars$hp / mtcars$cyl

# 7. It has been found that the cars' weight was overestimated by 5% for cars weighing more than 3.5 tons -- correct this
overweight_cars <- mtcars$wt > 3.5
mtcars$wt[overweight_cars] <- mtcars$wt[overweight_cars] / 1.05

# 8. install the "plyr" library, and repeat what you did in step 4 using the function ddply, a function in that library
install.packages("plyr")
library(plyr)
ddply(mtcars, .(cyl), function(x) mean(x$hp))

# 9. install the "ggplot2" library, and explore different visualisations of the data
p <- ggplot(mtcars, aes(wt, mpg))
p <- p + geom_point(aes(size = qsec, colour=hp, shape=factor(cyl))) +
         scale_colour_gradient(low = "blue", high="red") +
         ggtitle("a 5-dimensional graph")
print(p)

@
\fi

\section{Wrap-up Part I}
In this section, we have covered
\begin{itemize}
\item{Basic data types in R: vector, data.frame}
\item{Reading in data from a csv file}
\item{Using libraries and help}
\item{Subsetting a data.frame}
\end{itemize}

\part{rQTL}
\section{Installation}
To install rqtl, type \texttt{install.packages('qtl')}.
This package has been augmented by Julian Taylor -- one installs his add-on package by typing \texttt{install.packages('qtlPlus')}.

\section{Reading data into rqtl}
The rqtl package includes a function, \texttt{read.cross} that is designed to read a specially-formatted file containing qtl data, ready for analysis. When using read.cross, one must supply it with parameters that control its working.
\begin{enumerate}
\item{\texttt{file = "exmapOut.csv"} -- which file to load}
\item{\texttt{format = "csvr"} -- format of the file}
\item{\texttt{genotypes = c("AA", "BB")} -- they way the genotypes are labelled}
\item{\texttt{na.strings = c("-")} -- how missing data are labelled}
\item{\texttt{map.function = "kosambi"} -- how the mapping is performed}
\item{\texttt{estimate.map = FALSE} -- whether to estimate the map}
\end{enumerate}
More information can be found by typing \texttt{?read.cross} in R.

\subsection{Reading-in and interrogating qtl data}
\texttt{read.cross} produces a complicated "object"\footnote{we have talked about several objects in this workshop: lists, vectors, data.frames. What we haven't talked about, and what is out-of-scope, is that it is possible to create custom objects. Normally, a custom object will consist of a list of other objects, or a list of lists of other objects, etc... This exploits the fact that different objects can be mixed within a list}. With a small object, \texttt{str} can be useful to understand its internal structure, but that is less useful here because this object is too large.

<<qtl_1, eval=FALSE, cache=TRUE, result='asis',background='#FDFDFD'>>=
library(qtl)
library(qtlPlus)

# accessing the linkage map from the package
data(exmap, package = "qtlPlus")

# reading it in from a file
# there are a lot of potential arguments for this function but from experience users will require something
# similar to what I have below everytime

exmap <- read.cross(file = "qtl.dat", format = "csvr", genotypes = c("AA", "BB"), na.strings = c("-"), map.function = "kosambi", estimate.map = FALSE)

# printed breakdown of object
exmap


# number of markers on linkage groups
# number of linkage groups
# number of individuals
nmar(exmap)
nchr(exmap)
nind(exmap)

# structure of object is a list with two elements "pheno and "geno"
names(exmap)

# The "pheno" element is a data frame of phenotypes including a column of genotype names
# The "Genotype" column in the data frame SHOULD be a factor with levels equivalent ot the line names
exmap$pheno
names(exmap$pheno)
is.data.frame(exmap$pheno)
str(exmap$pheno)

# The geno element contains the linkage groups
names(exmap$geno)
names(exmap$geno$"1A")
str(exmap$geno$"1A")

# Each linkage group has two elements, "map" and "data".For each linkage group, the length of the "map"
# and the number of columns of the "data" will match each other. The rows exactly line up with the names
# in exmap$pheno$Genotype. R/qtl DOES NOT put row names on the data.
# looking at the marker data
head(exmap$geno$"1A"$data)
dim(exmap$geno$"1A"$data)
exmap$geno$"1A"$data[,"1A.m.10"]
dimnames(exmap$geno$"1A"$data)

# by default, R/qtl sets the allele codes as AA = 1, BB = 2. The object also has a class structure
class(exmap)

# the "bc" component of this cross represents a "backcross" so R/qtl knows this is a simple crossing
# of two parents (R/qtl handles more difficult crosses that are not discussed here).
@

Note that when one types \texttt{exmap} (or whatever one has called the object) the object's contents are not printed -- just a summary of them.

%%% PROBLEMS
\subsection{Exercises}
<<qtl_1_work1, eval=TRUE, cache=TRUE, result='asis', size="large",background='#D7FFD7', highlight=FALSE>>=
# 1. Install and load the required function libraries


# 2. read in qtl.dat using read.cross
exmap <- read.cross(file = "qtl.dat", format = "csvr", genotypes = c("AA", "BB"), na.strings = c("-"), map.function = "kosambi", estimate.map = FALSE)

# 3. How many individuals have any missing marker data?

@

<<qtl_1_work2, eval=TRUE, cache=TRUE, result='asis', size="large",background='#FfEDD9', highlight=FALSE>>= 

@

<<qtl_1_work3, eval=TRUE, cache=TRUE, result='asis', size="large",background='#FFDDDD', highlight=FALSE>>=
# nothing
@

%%% SOLUTIONS
\iftrainermanual
\subsection{Trainer solutions}
<<qtl_1_soln, eval=FALSE, cache=TRUE, result='asis', size="large",background='#DDFFFF', highlight=FALSE>>=
# 1. Install and load the required function libraries
install.packages("qtl")
install.packages("qtlPlus")
@

\subsection{Subsetting qtl data}
<<qtl_2, eval=FALSE, cache=TRUE, result='asis',background='#FDFDFD'>>=
# subsetting your data to linkage group to first three linkage groups
# this can be done in multiple different ways
names(exmap$geno)

# 1. By explicitly naming the linkage groups
exmaps <- subset(exmap, chr = c("1A","1B1","1B2"))
nmar(exmaps)

# 2. By using the names of the "geno" part of the object
exmaps <- subset(exmap, chr = names(exmap$geno)[1:3])
nmar(exmaps)

# 3. By setting up a logical vector
wchr <- names(exmap$geno) %in% names(exmap$geno)[1:3]
wchr
exmaps <- subset(exmap, chr = wchr)
nmar(exmaps)

# subsetting your data to linkage groups OTHER THAN the first three linkage groups,
exmaps <- subset(exmap, chr = !(names(exmap$geno) %in% names(exmap$geno)[1:3]))
nmar(exmaps)

# you can actually subset the map yourself without using R/qtl subset() command
exmaps <- exmap
exmaps$geno <- exmap$geno[1:3]
exmaps$geno <- exmap$geno[c("1A","1B1","1B2")]
nmar(exmaps)

exmapf <- exmap
exmapf$geno <- exmap$geno[4:length(exmap$geno)]
nmar(exmapf)

# and you can even put them back together
exmaps$geno <- c(exmapf$geno, exmaps$geno)
exmaps$geno <- exmap$geno[mixedorder(names(exmap$geno))]
nmar(exmaps)

### Subsetting individuals (i.e. removing genotypes from data)

# removing first three genotypes
# this can be done multiple ways

# 1. Stating numerically which ones to drop
exmaps <- subset(exmap, ind = -(1:3))
nind(exmaps)

# 2. Stating logically which ones to drop
whind <- !(exmap$pheno$Genotype %in% exmap$pheno$Genotype[1:3])
whind
exmaps <- subset(exmap, ind = whind)
nind(exmaps)

@

%%% PROBLEMS
\subsection{Exercises}
<<qtl_2_work1, eval=TRUE, cache=TRUE, result='asis', size="large",background='#D7FFD7', highlight=FALSE>>=
# 1. Create a new map that contains only marker data for the 1* chromasomes.

# 2. Create a new map that contains only individuals without any missing marker data

@

<<qtl_2_work2, eval=TRUE, cache=TRUE, result='asis', size="large",background='#FfEDD9', highlight=FALSE>>= 

# 2. Find any individuals possessing any markers that are not double haploid, and remove them from the object


@

<<qtl_2_work3, eval=TRUE, cache=TRUE, result='asis', size="large",background='#FFDDDD', highlight=FALSE>>=
# 4. Find any individuals possessing a phenotype that is more than 4 standard deviations from the mean and, remove them. (useful functions: var, sqrt)
@

%%% SOLUTIONS
\iftrainermanual
\subsection{Trainer solutions}
<<qtl_2_soln, eval=TRUE, cache=TRUE, result='asis', size="large",background='#DDFFFF', highlight=FALSE>>=
# nothing
@

\subsection{Other interrogation, modification and subsetting tools}
<<qtl_3, eval=FALSE, cache=TRUE, result='asis',background='#FDFDFD'>>=
# pull "map" component from each chromosome
pm <- pull.map(exmap)
pm

# check map of "1A" linkage groups
pm[["1A"]]

# drop markers and return object.
exmap$geno$"1A"$map
exmapd <- drop.markers(exmap, c("1A.m.1","1A.m.2"))
exmapd$geno$"1A"$map

# move marker from what linkage group to another and return object 
# (places marker at end of linkage group).
# this moves marker "1A.m1.1" to the end of linkage group "1B".
exmapm <- movemarker(exmap, "1A.m.1","1B")

# this switches the numeric values of the marker "1A.m.1" 
# (i.e. 1 becomes 2 and 2 becomes 1)
exmapm <- switchAlleles(exmap, "1A.m.1")

# returns chromosome and cm position of markers (find marker positions)
find.markerpos(exmap, c("1A.m.1","2A.m.7"))

### more advanced operations

# remove individuals with large recombination
countXO(map)
countXO(exmap)[countXO(exmap) > 40]
ct <- countXO(exmap) < 40
exmaps <- subset(exmap, ind = ct)

# reverse orientation of a linkage group
exmaps <- exmap
exmaps$geno$"1A"$map <- rev(exmaps$geno$"1A"$map)
exmaps$geno$"1A"$map <- abs(exmaps$geno$"1A"$map - exmaps$geno$"1A"$map[1])
exmaps$geno$"1A"$data <- exmaps$geno$"1A"$data[,ncol(exmaps$geno$"1A"$data):1]

# breaking linkage groups (qtlPlus function, see ?breakCross)
exmaps <- breakCross(exmap, split = list("1A" = "1A.m.4"))
nmar(exmaps)

# merging linkage groups (qtlPlus function, see ?mergeCross)
exmapz <- mergeCross(exmaps, merge = list("1A" = c("1A1","1A2")))
nmar(exmapz)
exmapz$geno$"1A"$map

# order linkage groups (uses mixedorder() from library gtools)
exmap$geno <- exmap$geno[mixedorder(names(exmap$geno))]


@

%%% PROBLEMS
\subsection{Exercises}
<<qtl_3_work1, eval=TRUE, cache=TRUE, result='asis', size="large",background='#D7FFD7', highlight=FALSE>>=
#nothing
@

<<qtl_3_work2, eval=TRUE, cache=TRUE, result='asis', size="large",background='#FfEDD9', highlight=FALSE>>= 

# find any markers with missing data in 4 or more individuals and remove them

@

<<qtl_3_work3, eval=TRUE, cache=TRUE, result='asis', size="large",background='#FFDDDD', highlight=FALSE>>=
# nothing
@

%%% SOLUTIONS
\iftrainermanual
\subsection{Trainer solutions}
<<qtl_3_soln, eval=TRUE, cache=TRUE, result='asis', size="large",background='#DDFFFF', highlight=FALSE>>=
# nothing
@





\section{Appendix}
\subsection{Installing R and RStudio}
On MS Windows, there are two steps:
\begin{enumerate}
\item{download \textit{R} from \url{http://cran.r-project.org/bin/windows/base/} and install it. }
\item{download \textit{RStudio desktop} from \url{https://www.rstudio.com/ide/download/} and install it}
\end{enumerate}

\end{document}
